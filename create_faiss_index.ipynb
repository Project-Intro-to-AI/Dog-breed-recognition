{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roboflow\n",
    "\n",
    "roboflow.login()\n",
    "\n",
    "roboflow.download_dataset(dataset_url=\"https://universe.roboflow.com/team-roboflow/coco-128/dataset/2\", model_format=\"coco\", location=\"data/coco-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, preprocess):\n",
    "        self.image_paths = image_paths\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.preprocess(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_data_paths(dir: str | list[str], data_formats: list, prefix: str = '') -> list[str]:\n",
    "    \"\"\"\n",
    "    Get list of files in a folder that have a file extension in the data_formats.\n",
    "\n",
    "    Args:\n",
    "      dir (str | list[str]): Dir or list of dirs containing data.\n",
    "      data_formats (list): List of file extensions. Ex: ['jpg', 'png']\n",
    "      prefix (str): Prefix for logging messages.\n",
    "\n",
    "    Returns:\n",
    "      A list of strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = []  # data files\n",
    "        for d in dir if isinstance(dir, list) else [dir]:\n",
    "            p = Path(d)\n",
    "            if p.is_dir():\n",
    "                f += glob.glob(str(p / '**' / '*.*'), recursive=True)\n",
    "            else:\n",
    "                raise FileNotFoundError(f'{prefix}{p} does not exist')\n",
    "        data_files = sorted(x for x in f if x.split('.')[-1].lower() in data_formats)\n",
    "        return data_files\n",
    "    except Exception as e:\n",
    "        raise Exception(f'{prefix}Error loading data from {dir}: {e}') from e\n",
    "\n",
    "\n",
    "def get_image_embeddings(data_dir, model_name=\"ViT-B/32\", batch_size=32, device=\"cpu\"):\n",
    "    # Load the CLIP model\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    # Create a dataset and dataloader\n",
    "    image_paths = get_data_paths(data_dir, data_formats=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    print(len(image_paths))\n",
    "    dataset = ImageDataset(image_paths, preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # List to store image embeddings\n",
    "    image_embeddings = []\n",
    "\n",
    "    # Process images in batches\n",
    "    with torch.no_grad():\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model.encode_image(images)\n",
    "            embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "            image_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    \n",
    "    return image_embeddings, image_paths\n",
    "\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    # Determine the dimensionality of the embeddings\n",
    "    d = 512\n",
    "    \n",
    "    # Initialize a FAISS index\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "def load_faiss_index(index_path, image_paths_path):\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load image paths\n",
    "    with open(image_paths_path, \"r\") as f:\n",
    "        image_paths = json.load(f)\n",
    "    \n",
    "    return index, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, image_paths = get_image_embeddings(\"data/coco-128/train\", device=\"cpu\")\n",
    "index = create_faiss_index(embeddings)\n",
    "faiss.write_index(index, \"data/index.faiss\")\n",
    "with open(\"data/image_paths.json\", \"w\") as f:\n",
    "    json.dump(image_paths, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_images(images, index, image_paths, top_k=5, model_name=\"ViT-B/32\", device=\"cpu\"):\n",
    "    # Load the CLIP model\n",
    "    clip_model, clip_preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    # Preprocess and get embeddings for input images\n",
    "    processed_images = torch.stack([clip_preprocess(img) for img in images]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = clip_model.encode_image(processed_images)\n",
    "        embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "    \n",
    "    # Search for similar images\n",
    "    D, I = index.search(embeddings, top_k)\n",
    "    \n",
    "    # Get the paths of the similar images\n",
    "    similar_images = [[image_paths[i] for i in indices] for indices in I]\n",
    "    print(similar_images)\n",
    "    similar_images = [[Image.open(img_path) for img_path in paths] for paths in similar_images]\n",
    "    \n",
    "    return similar_images\n",
    "\n",
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "index, image_paths = load_faiss_index(\"data/index.faiss\", \"data/image_paths.json\")\n",
    "\n",
    "\n",
    "query_images = [\"data/coco-128/test/000000000034_jpg.rf.b518abdaed199dcb88854cf20fce8078.jpg\", \n",
    "                \"data/coco-128/test/000000000283_jpg.rf.27927692baf616a7456bb3e24c21bfd7.jpg\"]\n",
    "query_images = [Image.open(img_path) for img_path in query_images]\n",
    "similar_images = get_similar_images(query_images , index, image_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cinhw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
